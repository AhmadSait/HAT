{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Scikit-Learn\n",
    "\n",
    "Today's workshop, which is presented by the [KAUST Visualization Core Lab (KVL)](https://corelabs.kaust.edu.sa/visualization/), is the first of two *Introduction to Machine Learning with Scikit-Learn* workshops. These workshops will largely follow Chapter 2 of [*Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow*](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) which walks through the process of developing an end-to-end machine learning project with [Scikit-Learn](https://scikit-learn.org/stable/index.html).\n",
    "\n",
    "## Today's schedule\n",
    "\n",
    "* Working with Real Data\n",
    "* Understanding the Big Picture\n",
    "* Getting the Data\n",
    "* Discovering and Visualizing the Data to Gain Insights\n",
    "\n",
    "## Next Thursday's Schedule\n",
    "\n",
    "* Preparing the Data for Machine Learning Algorithms\n",
    "* Selecting and Training a Model\n",
    "* Fine Tuning Your Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Real Data\n",
    "\n",
    "When you are just getting started with machine learning it is best to experiment with real-world data (as opposed to artificial data). The following are some good resources of open-source data that you can use for practice or research.\n",
    "\n",
    "* [University of California-Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n",
    "* [Kaggle](https://www.kaggle.com/datasets)\n",
    "* [OpenDataMonitor](http://opendatamonitor.eu/)\n",
    "* [Wikipedia's list of Machine Learning datasets](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)\n",
    "* [Datasets subreddit](https://www.reddit.com/r/datasets/)\n",
    "* [Quora's list of open datasets](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)\n",
    "\n",
    "Major cloud providers all have repositories of publically available datasets.\n",
    "\n",
    "* [Open Data on AWS](https://registry.opendata.aws/)\n",
    "* [Open Data on GCP](https://cloud.google.com/public-datasets/)\n",
    "* [Open Data on Azure](https://azure.microsoft.com/en-us/services/open-datasets/)\n",
    "\n",
    "Finally, [Pandas DataReader](https://pydata.github.io/pandas-datareader/) provides a unified API to a [number of datasets](https://pydata.github.io/pandas-datareader/remote_data.html). Note that many of these data sources require you to create an account and get an API key.\n",
    "\n",
    "## 1990 U.S. California Census Data\n",
    "\n",
    "Today we will be working with a 1990 U.S. Census dataset from California. This dataset contains socioeconomic data on groups of California residents include. While this dataset is a bit \"dated\" it will allow us to explore many different aspects of a typical machine learning project.\n",
    "\n",
    "The California census data includes metrics such as population, median income, median house prices for each census block group in the state of California. Census block groups are the smallest geographical unit for which the U.S. Census Bureau publishes public data (for reference a census block group typically has a population of between 600 and 3000 people). Sometime census block groups are called districts for short. The figure below (reproduced from *Chapter 2* of *Hands on Machine Learning with Scikit-Learn, Keras, and TensorFlow*).\n",
    "\n",
    "<center><img src=\"./assets/figure-2-1.png\" alt=\"Figure 2.1\" title=\"California housing used to be affordable!\" align=\"center\" width=\"750\" height=\"750\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the Big Picture\n",
    "\n",
    "Our goal over these two workshops will be to build a machine learning modeling pipeline that uses the California census data to predict housing prices. Today we will mostly focus on getting the data, exploring the data to gain insights,. Believe it or not these initial steps are what data scientists and machine learning engineers spend the majority of their time doing! \n",
    "\n",
    "Next Thursday we will prepare our data for machine learning see how to fit a variety of machine learning models to our dataset and shortlist a few candidate models for further analysis. We will then use hyper-parameter tuning to improve the performance of our shortlisted models to arrive at an overall \"best\" model. We will finish with a discussion of model how to present the results of your model and talk about some of the aspects of deploying a trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing the problem\n",
    "\n",
    "### What is the business/research objective?\n",
    "\n",
    "Typically building the model is *not* the overall objective but rather the model itself is only one part of a larger process used to answer a business/research question. Knowing the overall objective is important because it will determine your choice of machine learning algorithms to train, your measure(s) of model performance, and how much time you will spend tweaking the hyper-parameters of your model.\n",
    "\n",
    "In our example today, the overall business objective is to make money investing in houses. Our house price prediction model with be one of potentially many other models whose predictions are taken as inputs into another machine learning model that will be used to determine whether or not investing in houses in a particular area is a good idea. This hypothetical ML pipeline is shown in the figure below (reproduced from *Chapter 2* of *Hands on Machine Learning with Scikit-Learn, Keras, and TensorFlow*).\n",
    "\n",
    "<center><img src=\"./assets/figure-2-2.png\" alt=\"Figure 2.2\" title=\"Hypothetical ML pipeline\" align=\"center\" width=\"1000\" height=\"250\" /></center>\n",
    "\n",
    "\n",
    "### What is the *current* solution?\n",
    "\n",
    "Always a good idea to know what the current solution to the problem you are trying to solve. Current solution gives a benchmark for performance. Note that the current \"best\" solution could be *very* simple or could be *very* sophisticated. Understanding the current solution helps you think of a good place to start. Example: suppose that the current solution for predicting the price of a house in a given census block is to ignore all the demographic information and predict a simple average of house prices in nearby census blocks. In this case it would probably *not* make sense to start building a complicated [deep learning model](https://en.wikipedia.org/wiki/Deep_learning) to predict housing prices. However, if the current solution was a tuned [gradient boosted machine](https://en.wikipedia.org/wiki/Gradient_boosting) then it probably would *not* make sense to try a much simpler [linear regression](https://en.wikipedia.org/wiki/Linear_regression) model.\n",
    "\n",
    "With all this information, you are now ready to start designing your system. First, you need to frame the problem by answering the following questions.\n",
    "\n",
    "* Is our problem [supervised](https://en.wikipedia.org/wiki/Supervised_learning), [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning), or [reinforcement](https://en.wikipedia.org/wiki/Reinforcement_learning) learning?\n",
    "* Is our problem a [classification](https://en.wikipedia.org/wiki/Statistical_classification) task, a [regression](https://en.wikipedia.org/wiki/Regression_analysis) task, or something else? If our problem is a classification task are we trying to classify samples into 2 categories (binary classification) or more than 2 (multi-class classification) categories? If our problem is a regression task, are we trying to predict a single value (univariate regression) or multiple values (multivariate regression) for each sample? \n",
    "* Should you use batch learning or [online learning](https://en.wikipedia.org/wiki/Online_machine_learning) techniques?\n",
    "\n",
    "## Select a Performance Measure\n",
    "\n",
    "Next we want to select a performance measure. A typical performance measure for regression problems of this type is the root mean square error (RMSE).\n",
    "\n",
    "$$ \\mathrm{RMSE}(X, h) = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (h(x_i) - y_i)^2} $$\n",
    "\n",
    "In the above formula $X$ is the dataset, $h$ is the decision function or model (which we will train on the dataset!), $x_i$ is the $i$ training sample from the dataset (variables used to predict house prices in our problem), $y_i$ is the $i$ target value from the dataset (house prices in order problem).\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Scikit-Learn has a number of different [possible metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) that you can choose from (or you can create your own custom metric if required). Can you find at least one other metric that seems appropriate for our house price prediction model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Assumptions!\n",
    "\n",
    "Always a good idea to check any assumptions that you have made about either the inputs to your machine learning model or about how the outputs of your machine learning model will be used. Currently we have formulated our machine learning problem as a *supervised*, *univariate regression* problem suitable for *batch* learning. What if the downstream model doesn't actually use the numerical predictions from our model but rather converts the predicted prices into categories \"cheap\", \"fair\", \"expensive\"? Maybe it would be better to change the design of our machine learning model so that it predicts these categories directly? If so, then we would need to use completely different algorithms as the machine learning problem would then be a classification problem and not a regression problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data\n",
    "\n",
    "Time to get our hands dirty with the data!\n",
    "\n",
    "## Create the Workspace\n",
    "\n",
    "For this training the workspace has already been created for you so there is nothing to install! However, if you would like to understand how to install the software stack used in this training then please see the instructions in the [README](../README.md) file of this repository. There are three files that define the software dependencies to be installed.\n",
    "\n",
    "* [`environment.yml`](../environment.yml) for [Conda](https://docs.conda.io/en/latest/) installed dependencies.\n",
    "* [`requirements.txt`](../requirements.txt) for [Pip](https://pip.pypa.io/en/stable/) installed dependencies.\n",
    "* [`postBuild`](../postBuild) which contains instructions for installing any required [Jupyter](https://jupyter.org/) extensions. \n",
    "\n",
    "The process for creating the virtual environment has been automated using the [`create-conda-env.sh`](../bin/create-conda-env.sh) script.\n",
    "\n",
    "For more on using Conda (+pip) to create virtual environments and manage your data science and machine learning software stacks you can check out our [Introduction to Conda for (Data) Scientists](https://carpentries-incubator.github.io/introduction-to-conda-for-data-scientists/) training materials and watch our recent training workshops on the KAUST Visualization Core Lab (KVL) [YouTube Channel](https://www.youtube.com/channel/UCR1RFwgvADo5CutK0LnZRrw).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "HOUSING_URL = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.tgz\"\n",
    "\n",
    "\n",
    "def fetch_data():\n",
    "    if not os.path.isfile('../data/housing/housing.tgz'):\n",
    "        os.makedirs(\"../data/housing/\", exist_ok=True)\n",
    "        urllib.request.urlretrieve(HOUSING_URL, \"../data/housing/housing.tgz\")\n",
    "    \n",
    "    with tarfile.open(\"../data/housing/housing.tgz\") as tf:\n",
    "        tf.extractall(path=\"../data/housing/\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will load the data using the [Pandas](https://pandas.pydata.org/) library. Highly recommend the most recent edition of [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781491957653/) by Pandas creator Wes Mckinney for anyone interested in learning how to use Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "housing_df = pd.read_csv(\"../data/housing/housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a quick look at the data\n",
    "\n",
    "Each row represents a census block group or district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we have any non-numeric attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(housing_df.loc[:, \"ocean_proximity\"]\n",
    "           .value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms of your numeric attributes\n",
    "\n",
    "Another good way to get a feel for your data is to plot histograms of your numerical attributes. Histograms show the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). Can plot histograms of attributes one at a time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "(housing_df.loc[:, \"median_house_value\"]\n",
    "           .hist(bins=50, ax=ax))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or can call the [`hist`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html) method on the entire `DataFrame`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(20,15))\n",
    "housing_df.hist(bins=50, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Do you notice anything unusual or interesting about the above histrograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "1. The `median_income` attribute does not appear to be expressed in USD. Maybe 10k USD? Important to always know the units of the attributes in your dataset. Also the values appear to be capped at both the lower and upper ends. Working with processed data is very common in machine learning and is not necessarily a problem (you just need to be aware of how the raw data was transformed).\n",
    "2. Looks like the `housing_median_age` and `median_house_value` were also capped at the upper end. The latter *might* be a serious problem as house prices is the target that we are trying to predict. What to do? First, check with teams downstream (i.e., teams using your prediction as an input to their own model/process). If they need predictions that go beyond 500k USD then you have two options: either collect proper target values for the data (may not be feasible!) or drop the training samples from the dataset (so your model will not be evaulated poorly for predicting prices greater than 500k USD).\n",
    "3. Attributes have very different scales! `housing_median_age` ranges from 0 to 50 but `total_rooms` ranges from 0 to 40000! Many machine learning algorithms will perform poorly when input attributes have wildly different scales. We will discuss this later when we come to feature scaling.\n",
    "4. Many of these histograms are skewed to the right (i.e., have a \"heavy\" tail). Theory underlying many machine learning algorithms often assumes that attributes have Normal/Gaussian, \"Bell-shaped\" distribution. Maybe we can find some tranformations that will make these attributes appear to be more \"Bell-shaped.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Test Set\n",
    "\n",
    "Before we look at the data any further, we need to create a test set, put it aside, and never look at it (until we are ready to test our trainined machine learning model!). Why? We don't want our machine learning model to memorize our dataset (this is called overfitting). Instead we want a model that will generalize well (i.e., make good predictions) for inputs that it didn't see during training. To do this we hold split our dataset into training and testing datasets. The training dataset will be used to train our machine learning model(s) and the testing dataset will be used to make a final evaluation of our machine learning model(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you might refresh data in the future...\n",
    "\n",
    "...then you want to use some particular hashing function to compute the hash of a unique identifier for each observation of data and include the observation in the test set if resulting hash value is less than some fixed percentage of the maximum possible hash value for your algorithm. This way even if you fetch more data, your test set will never include data that was previously included in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "\n",
    "def in_testing_data(identifier, test_size):\n",
    "    _hash = zlib.crc32(bytes(identifier))\n",
    "    return _hash & 0xffffffff < test_size * 2**32\n",
    "\n",
    "\n",
    "def split_train_test_by_id(data, test_size, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda identifier: in_testing_data(identifier, test_size))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If this is all the data you will ever have...\n",
    "\n",
    "...then you can just set a seed for the random number generator and then randomly split the data. Scikit-Learn has a [`model_selection`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) module that contains tools for splitting datasets into training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "_seed = 42 \n",
    "_random_state = np.random.RandomState(_seed)\n",
    "training_data, testing_data = model_selection.train_test_split(housing_df, test_size=0.2, random_state=_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = (housing_df.loc[:, \"median_income\"]\n",
    "               .hist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are told that `median_income` is a very important feature which determines house prices. We might want to make sure that the distribution of `median_income` in our testing dataset closely matches the distribution of `median_income` in our entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current distribution of median income in testing dataset is not quite the same as in the overall dataset\n",
    "_ = (testing_data.loc[:, \"median_income\"]\n",
    "                 .hist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can accomplish this by creating a temporary variable that bins samples according to the value of `median_income`. This technique is called [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = 42\n",
    "_prng = np.random.RandomState(_seed)\n",
    "_median_income_strata = pd.cut(housing_df.loc[:, \"median_income\"], \n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf], \n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "training_data, testing_data = model_selection.train_test_split(housing_df,\n",
    "                                                               test_size=0.2,\n",
    "                                                               random_state=_prng,\n",
    "                                                               stratify=_median_income_strata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = (testing_data.loc[:, \"median_income\"]\n",
    "                 .hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = (housing_df.loc[:, \"median_income\"]\n",
    "               .hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where possible I like to write out the training and testing data sets to disk\n",
    "training_data.to_csv(\"../data/housing/training.csv\", index_label=\"id\")\n",
    "testing_data.to_csv(\"../data/housing/testing.csv\", index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and visualize the data to gain insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "_ = training_data.plot.scatter(x=\"longitude\", y=\"latitude\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "_ = training_data.plot.scatter(x=\"longitude\", y=\"latitude\", ax=ax, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "_size = (training_data.loc[:, \"population\"]\n",
    "                      .div(100))\n",
    "_color = training_data.loc[:, \"median_house_value\"]\n",
    "_cmap = plt.get_cmap(\"viridis\")\n",
    "\n",
    "_ = (training_data.plot\n",
    "                  .scatter(x=\"longitude\", y=\"latitude\", ax=ax, alpha=0.4, s=_size, c=_color, cmap=_cmap, label=\"population\", colorbar=True))\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for correlations\n",
    "\n",
    "Since our dataset is not too large we can compute the [correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient) between every pair of attributes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations between attributes are our target\n",
    "(training_data.corr()\n",
    "              .loc[:, \"median_house_value\"]\n",
    "              .sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation coefficient only measures *linear* correlations\n",
    "\n",
    "Correlation coefficient only captures *linear* relationships between variables (i.e., if $x$ goes up, then $y$ generally goes up/down). It may completely miss any non-linear relationships between variables. The figure below (reproduced from *Chapter 2* of *Hands on Machine Learning with Scikit-Learn, Keras, and TensorFlow*) shows correlation coefficients for various datasets. Check out the bottom row!\n",
    "\n",
    "<center><img src=\"./assets/figure-2-14.png\" alt=\"Figure 2.14\" title=\"Correlation coefficient only captures linear dependence\" align=\"center\" width=\"750\" height=\"750\" /></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import plotting\n",
    "\n",
    "\n",
    "attributes = [\"median_house_value\",\n",
    "              \"median_income\",\n",
    "              \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "_ = plotting.scatter_matrix(training_data.loc[:, attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "_ = training_data.plot.scatter(x=\"median_income\", y=\"median_house_value\", alpha=0.1, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with attribute combinations\n",
    "\n",
    "One last thing you may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations. This process is called *feature engineering*. Understanding how to engineer features that are useful for solving a particular machine learning problem often requires a significant amount of domain expertise. However a number of libraries, such as [featuretools](https://www.featuretools.com/), have been developed to (partially) automate the process of feature engineering. Also the success of deep learning in many fields is in large part due to the fact that deep neural networks engineer their own features during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rooms_per_household = (training_data.loc[:, \"total_rooms\"]\n",
    "                                     .div(training_data.loc[:, \"households\"]))\n",
    "\n",
    "_bedrooms_per_room = (training_data.loc[:, \"total_bedrooms\"]\n",
    "                                   .div(training_data.loc[:, \"total_rooms\"]))\n",
    "\n",
    "_population_per_household = (training_data.loc[:, \"population\"]\n",
    "                                          .div(training_data.loc[:, \"households\"]))\n",
    "\n",
    "new_attributes = {\"rooms_per_household\": _rooms_per_household,\n",
    "                  \"bedrooms_per_room\": _bedrooms_per_room, \n",
    "                  \"population_per_household\": _population_per_household}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations are now stronger\n",
    "(training_data.assign(**new_attributes)\n",
    "              .corr()\n",
    "              .loc[:, \"median_house_value\"]\n",
    "              .sort_values(ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
