{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Scikit-Learn\n",
    "\n",
    "Today's workshop, which is presented by the [KAUST Visualization Core Lab (KVL)](https://corelabs.kaust.edu.sa/visualization/), is the second of two *Introduction to Machine Learning with Scikit-Learn* workshops. These workshops will largely follow Chapter 2 of [*Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow*](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) which walks through the process of developing an end-to-end machine learning project with [Scikit-Learn](https://scikit-learn.org/stable/index.html).\n",
    "\n",
    "## Today's schedule\n",
    "\n",
    "* Preparing the Data for Machine Learning Algorithms\n",
    "* Selecting and Training a Model\n",
    "* Fine Tuning Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for machine learning algorithms\n",
    "\n",
    "\"Best practice\" is to write functions to automate the process of preparing your data for machine learning. Why?\n",
    "\n",
    "* Allows you to reproduce these transformations easily on any dataset.\n",
    "* You will gradually build a library of transformation functions that you can reuse in future projects.\n",
    "* You can use these functions in a \"live\" system to transform the new data before feeding it to your algorithms.\n",
    "* This will make it possible for you to easily experiment with various transformations and see which combination of transformations works best.\n",
    "\n",
    "First we need to load the training data. The code below loads the training dataset that we created last week using stratified sampling on binned value of `median_income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv(\"../data/housing/training.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We are going to start with some basic feature engineering and data cleaning tasks that we discussed in last week's session but that we didn't actually complete. Feature engineering is one of the most important parts of any machine learning project. Feature engineering is often the most labor intensive part of building a machine learning pipeline and often requires extensive expertise/domain knowledge relevant to the problem at hand.\n",
    "\n",
    "Recently packages such as [featuretools](https://www.featuretools.com/) have been developed to (partially) automate the process of feature engineering. The success of [deep learning](https://en.wikipedia.org/wiki/Deep_learning) in various domains is in significant part due to the fact that deep learning models are able to automatically engineer features that are most useful for solving certain machine learning tasks. In effect deep learng replaces the expensive to acquire expertise/domain knowledge required to hand-engineer predictive features. The story about [space2vec](https://medium.com/dessa-news/space-2-vec-fd900f5566), a deep learning based supernovae classifier developed by machine learning engineers with no expertise in Astronomy that was able to outperform the machine learning solution developed by NERSC scientists, is a recent example of the power of automated feature engineering. The machine learning pipeline developed by NERSC scientists, called [AUTOSCAN](https://portal.nersc.gov/project/dessn/autoscan/), was a significant improvement over the previous solution which relied on manual classification of supernovae by astronomers. However, in order to achieve such high accuracy, the NERSC solution relied on a dataset of hand-engineered features developed by astronomers with over a century of combined training and expertise in the domain. The deep learning algorithm used by space2vec could be applied directly to the raw image data and did not rely on any hand-engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Encapsulate feature engineering in a function so it can be easiyl applied to training and testing datasets.\"\"\"\n",
    "    _rooms_per_household = (df.loc[:, \"total_rooms\"]\n",
    "                              .div(df.loc[:, \"households\"]))\n",
    "\n",
    "    _bedrooms_per_room = (df.loc[:, \"total_bedrooms\"]\n",
    "                            .div(df.loc[:, \"total_rooms\"]))\n",
    "\n",
    "    _population_per_household = (df.loc[:, \"population\"]\n",
    "                                   .div(df.loc[:, \"households\"]))\n",
    "\n",
    "    new_attributes = {\"rooms_per_household\": _rooms_per_household,\n",
    "                      \"bedrooms_per_room\": _bedrooms_per_room, \n",
    "                      \"population_per_household\": _population_per_household}\n",
    "    return df.assign(**new_attributes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_with_extra_features = engineer_features(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_with_extra_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_with_extra_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the target variable `median_house_value` as well as attributes `housing_median_age` and `median_income` are all truncated above some threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 8))\n",
    "_ = (training_df_with_extra_features.loc[:, [\"housing_median_age\", \"median_income\", \"median_house_value\"]]\n",
    "                                    .hist(bins=50, ax=ax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to drop all the observations whose values for at least one of these variables match their respective maximum values. We are also going to encapsulate the logic for dropping observations in a function so that we can reuse the same logic later to drop values from the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drop_max_values(df, attribute):\n",
    "    threshold = (df.loc[:, attribute]\n",
    "                   .max())\n",
    "    return df.loc[df.loc[:, attribute] < threshold, :]\n",
    "\n",
    "\n",
    "def clean_dataset(df):\n",
    "    \"\"\"\n",
    "    * Median house values were truncated at 500000 USD. Census block groups with median house values \n",
    "      equal to this threshold should be excluded from the analysis.\n",
    "    * Median income values were truncated at 15 (thousand USD). Census block groups with median income\n",
    "      values equal to this threshold should be excluded from the analysis.\n",
    "    * Median housing ages were truncated at 52 years. Census block groups with housing median age \n",
    "      values equal to this threshold should be excluded from the analysis.\n",
    "    \n",
    "    \"\"\"\n",
    "    _df = _drop_max_values(df, \"median_house_value\")\n",
    "    _df = _drop_max_values(_df, \"median_income\")\n",
    "    _df = _drop_max_values(_df, \"housing_median_age\")\n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_training_df = clean_dataset(training_df_with_extra_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 8))\n",
    "_ = (cleaned_training_df.loc[:, [\"housing_median_age\", \"median_income\", \"median_house_value\"]]\n",
    "                        .hist(bins=50, ax=ax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also separate the attributes/features and the labels/targets. Separating the attributes/features from the labels/targets allows us to more easily apply different sets of transformations to these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_df = cleaned_training_df.drop(\"median_house_value\", axis=1)\n",
    "training_target_df = cleaned_training_df.loc[:, [\"median_house_value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms will not work with missing data. There are three options for dealing with missing data.\n",
    "\n",
    "1. Drop any training samples that are missing values for *any* attribute/feature.\n",
    "2. Drop any attribute/feature with missing values.\n",
    "3. Explicitly decide how to fill in the missing values.\n",
    "\n",
    "We can implement any of the above approaches using built-in functionality of Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "(training_features_df.dropna(subset=[\"total_bedrooms\"])\n",
    "                     .info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2\n",
    "(training_features_df.drop(\"total_bedrooms\", axis=1)\n",
    "                     .info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3\n",
    "_median = (training_features_df.loc[:, \"total_bedrooms\"] # save this value for later so you can prepare the testing features!\n",
    "                               .median())\n",
    "(training_features_df.fillna({\"total_bedrooms\": _median})\n",
    "                     .info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, rather than using Pandas I recommend using the [Scikit-Learn](https://scikit-learn.org/stable/index.html). The Scikit-Learn [`impute`](https://scikit-learn.org/stable/modules/impute.html) module contains a number of different algorithms for filling missing values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import impute\n",
    "\n",
    "\n",
    "simple_imputer = impute.SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`impute.SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) is the first Scikit-Learn Transformer that we have encountered. As such now is a good to to discuss the Scikit-Learn application programming interface (API). The Scikit-Learn API is one of the best designed API's around and has heavily influenced API design choices of other libraries in the Python Data Science and Machine Learning ecosystem, in particular [Dask](https://dask.org/) and [NVIDIA RAPIDS](https://rapids.ai/index.html). Familiarly with the Scikit-Learn API will make it easier for you to get started with these libraries.\n",
    "\n",
    "The Scikit-Learn API is built around the following key concepts.\n",
    "\n",
    "* Estimators: Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an `impute.SimpleImputer` is an estimator). The estimation itself is performed by the `fit` method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a *hyperparameter* (such as the `strategy` parameter in `impute.SimpleImputer`), and it must be set as an instance variable (generally via a constructor parameter).\n",
    "\n",
    "* Transformers: Some estimators (such as an `impute.SimpleImputer`) can also transform a dataset; these are called transformers. Once again, the API is simple: the transformation is performed by the `transform` method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for an imputer. All transformers also have a convenience method called `fit_transform` that is equivalent to calling `fit` and then `transform` (but sometimes `fit_transform` is optimized and runs much faster).\n",
    "\n",
    "* Predictors: Finally, some estimators, given a dataset, are capable of making predictions; they are called predictors. A predictor has a `predict` method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a `score` method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms).\n",
    "\n",
    "All of an estimator’s hyperparameters are accessible directly via public instance variables (e.g., `simple_imputer.strategy`), and all the estimator’s learned parameters are accessible via public instance variables with an underscore suffix (e.g., `simple_imputer.statistics_`). Finally, Scikit-Learn provides reasonable default values for most parameters which makes it easy to quickly create a baseline working system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer.fit(training_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the median only exists for numeric atttributes/features, you will need to drop all of the non-numeric attributes/features from the dataset before fitting `simple_imputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_df = training_features_df.drop(\"ocean_proximity\", axis=1)\n",
    "simple_imputer.fit(numeric_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the `simple_impute` will compute the median values for each attribute/feature in the dataset and store the values for later reuse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medians computed using Pandas give same results as above\n",
    "numeric_features_df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill any missing value in the original dataset using the median values computed by calling the `fit` method, we call the `tranform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_numeric_features_df = simple_imputer.transform(numeric_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z is numpy array and no longer has any missing values\n",
    "np.any(imputed_numeric_features_df == np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a `fit_transform` method which combines the calls to `fit` and `transform` in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_numeric_features_df = simple_imputer.fit_transform(numeric_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes\n",
    "\n",
    "So far we have only discussed how to handle numeric attributes/features. Our dataset contains on non-numeric attribute/feature, `ocean_proximity` which we have good reason to think is important determinant of housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_features_df = training_features_df.loc[:, [\"ocean_proximity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above might look like arbitrary text, `ocean_proximity` only takes a limited number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_features_df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms almost always work with numbers. The Scikit-Learn [`preprocessing`](https://scikit-learn.org/stable/modules/preprocessing.html) module has several strategies for [encoding non-numeric attributes/features](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features). The simplest strategy is called ordinal encoding and is implemented by the [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "ordinal_encoder = preprocessing.OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = ordinal_encoder.fit_transform(non_numeric_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this representation machine learning algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases, for example cases where the the categories have a natural ordering such as “bad,” “average,” “good,” and “excellent”. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Can anyone see an issue with using an ordinal encoding for our `ocean_proximity` attribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The categories for `ocean_proximity` are not obviously ordered. For example category `0` (`<1H Ocean`) and category `4` (`NEAR OCEAN`) are cleary more similar than to categories `1` and `3`, respectively. Also what about the category `3` (`ISLAND`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative encoding strategy that is commonly used with categorical features that have not natural ordering is to create one binary attribute per category. In our case we create one attribute equal to `1` when the category is `<1H OCEAN` (and `0` otherwise), another attribute equal to `1` when the category is `INLAND` (and `0` otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). These new attributes are sometimes called dummy attributes. Scikit-Learn provides a [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) class to convert categorical values into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = preprocessing.OneHotEncoder()\n",
    "Z = one_hot_encoder.fit_transform(non_numeric_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed features are now a sparse matrix\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to dense numpy array \n",
    "Z.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if a categorical attribute has a large number of possible categories, then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. If this happens, you may want to try replacing the categorical attributes/features with useful numerical attributes/features related to the categories: for example, you could replace the `ocean_proximity` feature with the distance to the ocean. Alternatively, you could replace each category with a learnable, low-dimensional vector called an embedding. This approach is called [feature learning](https://en.wikipedia.org/wiki/Feature_learning) or representation learning and is covered in chapters 13 and 17 of textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Machine learning algorithms typically don’t perform well when the input numerical attributes have very different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest approach is to rescale features so that they all reside within the same range (typically between 0 and 1). This approach is implemented in Scikit-Learn by the [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_numeric_features_df = min_max_scaler.fit_transform(imputed_numeric_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler.data_min_ # these values will be reused later to rescale the testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler.data_max_ # these values will be reused later to rescale the testing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens if an attribute has outliers and you apply min-max scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = training_features_df.plot(kind=\"box\", subplots=True, figsize=(24, 8))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is to rescale features so that they all have zero mean and unit standard deviation. This approach, which is also called standardization, is particularly useful when attributes/features have outliers and when downstream machine learning algorithms assume that attributes/features have a Gaussian or Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this to make sure that all numerical features have the same scale\n",
    "standard_scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_numeric_features_df = standard_scaler.fit_transform(imputed_numeric_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler.mean_ # these values will be reused later to rescale the testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler.scale_ # these values will be reused later to rescale the testing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipelines\n",
    "\n",
    "As you can see creating preprocessing pipelines involves quite a lot of steps and each of the steps needs to be executed in the correct order. Fortunately Scikit-Learn allows you to combine estimators together to create [pipelines](https://scikit-learn.org/stable/modules/compose.html#combining-estimators). We can encapsulate all of the preprocessing logic for our numeric attributes as well as the preprocessing logic for our non-numeric attributes into separate instances of the [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) class.\n",
    "\n",
    "The `Pipeline` constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a `fit_transform` method). The names can be anything you like (as long as they are unique). Later we will see how to access the parameters of pipelines using these names when we discuss hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "\n",
    "\n",
    "numerical_pipeline = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy=\"median\")),\n",
    "        ('standard_scaler', preprocessing.StandardScaler())\n",
    "    ],\n",
    ")\n",
    "\n",
    "categorical_pipeline = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\"one_hot_encoder\", preprocessing.OneHotEncoder())\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then [combine these pipelines](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data) into a single pipeline using the [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer) class. The constructor requires a list of tuples, where each tuple contains a name, a transformer, and a list of names (or indices) of columns that the transformer should be applied to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import compose\n",
    "\n",
    "\n",
    "numerical_attributes = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "    \"rooms_per_household\",\n",
    "    \"bedrooms_per_room\",\n",
    "    \"population_per_household\",\n",
    "]\n",
    "\n",
    "categorical_attributes = [\n",
    "    \"ocean_proximity\"\n",
    "]\n",
    "\n",
    "preprocessing_pipeline = compose.ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numerical_pipeline\", numerical_pipeline, numerical_attributes),\n",
    "        (\"categorical_pipeline\", categorical_pipeline, categorical_attributes)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the entire preprocessing pipeline to our training features dataset in one go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_training_features = preprocessing_pipeline.fit_transform(training_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(preprocessed_training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I often find it useful to create a Pandas `DataFrame` from the `preprocessed_training_features` NumPy `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(preprocessing_pipeline.named_transformers_[\"categorical_pipeline\"]\n",
    "                                        .named_steps[\"one_hot_encoder\"]\n",
    "                                        .categories_[0])\n",
    "\n",
    "_columns = numerical_attributes + categories\n",
    "preprocessed_training_features_df = (pd.DataFrame\n",
    "                                       .from_records(preprocessed_training_features, columns=_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_training_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful class is [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion). `FeatureUnion` combines several transformer objects into a new transformer that combines their output. A `FeatureUnion` takes a list of transformer objects. During fitting, each of these transformers is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, estimators can be displayed with a HTML representation when shown in a Jupyter notebook. Visualizing estimators is particularly useful to diagnose or visualize a `Pipeline` with many estimators. This visualization is activated by setting the display option in [sklearn.set_config](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "\n",
    "set_config(display='diagram') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select and Train a Model\n",
    "\n",
    "At last! You framed the problem, you got the data and explored it, you sampled a training set and a test set, and you wrote transformation pipelines to clean up and prepare your data for machine learning algorithms automatically. You are now ready to select and train a Machine Learning model. You might have been wondering if we were every going to make it to this point! Fact is, most of your time developing machine learning solutions to real-world problems will not be spent training machine learning models: most of *your* time will be spent preparing the data for machine learning algorithms and most of the *computer* time will be spent training the machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating on the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "regressor = linear_model.LinearRegression()\n",
    "regressor.fit(preprocessed_training_features, training_target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have fit your first machine learning model using Scikit-Learn. Now let's evaluate our model's performance using our chosen metric: root mean squared error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "predictions = regressor.predict(preprocessed_training_features)\n",
    "mse = metrics.mean_squared_error(training_target_df, predictions)\n",
    "rmse = mse**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse # units are USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is often a sensible model to start but often underfits datasets with more complex relationships. Let’s train a [`tree.DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). This is a powerful model, capable of finding complex nonlinear relationships in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "\n",
    "regressor = tree.DecisionTreeRegressor()\n",
    "regressor.fit(preprocessed_training_features, training_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.predict(preprocessed_training_features)\n",
    "mse = metrics.mean_squared_error(training_target_df, predictions)\n",
    "rmse = mse**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what!? No error at all? Could this model really be absolutely perfect? Unfortunately it is much more likely that the model has badly overfit the training data. How can you be sure? As we saw earlier, you don’t want to touch the testing dataset until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part of it for model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Evaluation using Cross Validation\n",
    "\n",
    "The following code use Scikit-Learn [`model_selection.cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) to randomly split the training set into 10 distinct subsets called folds, then it trains and evaluates our model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "linear_regression_scores = model_selection.cross_val_score(linear_model.LinearRegression(),\n",
    "                                                           X=preprocessed_training_features,\n",
    "                                                           y=training_target_df,\n",
    "                                                           cv=10,\n",
    "                                                           scoring=\"neg_mean_squared_error\",\n",
    "                                                           n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rmses(rmses):\n",
    "    print(\"RMSE mean:\", rmses.mean())\n",
    "    print(\"RMSE standard deviation:\", rmses.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_rmses = np.sqrt(-linear_regression_scores)\n",
    "display_rmses(linear_regression_rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_random_state = np.random.RandomState(42)\n",
    "decision_tree_scores = model_selection.cross_val_score(tree.DecisionTreeRegressor(random_state=_random_state),\n",
    "                                                       X=preprocessed_training_features,\n",
    "                                                       y=training_target_df,\n",
    "                                                       cv=10,\n",
    "                                                       scoring=\"neg_mean_squared_error\",\n",
    "                                                       n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_rmses = np.sqrt(-decision_tree_scores)\n",
    "display_rmses(decision_tree_rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `DecisionTreeRegressor` doesn’t look nearly as good as it did earlier. In fact, it seems to perform worse than the much simpler `LinearRegression` model. Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try one last model now: the [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Random forests work by training many decision trees on random subsets of the features, then averaging the predictions made by each of the decision trees to arrive at an overall prediction. Building a model on top of many other models is called [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) and it is often a great approach to improve the predictions of your machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "_random_state = np.random.RandomState(42)\n",
    "regressor = ensemble.RandomForestRegressor(random_state=_random_state)\n",
    "regressor.fit(preprocessed_training_features, training_target_df.iloc[:, 0].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.predict(preprocessed_training_features)\n",
    "mse = metrics.mean_squared_error(training_target_df, predictions)\n",
    "rmse = mse**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_random_state = np.random.RandomState(42)\n",
    "random_forest_scores = model_selection.cross_val_score(ensemble.RandomForestRegressor(random_state=_random_state),\n",
    "                                                       X=preprocessed_training_features,\n",
    "                                                       y=training_target_df,\n",
    "                                                       cv=10,\n",
    "                                                       scoring=\"neg_mean_squared_error\",\n",
    "                                                       n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_rmses = np.sqrt(-random_forest_scores)\n",
    "display_rmses(random_forest_rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `RandomForestRegressor` look very promising. Note that the score on the training set is still much lower than on the validation sets which indicates that this model is still overfitting the training set. Possible solutions for overfitting are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Before we dive into hyperparameter tuning, you should out a few other models from various categories of machine Learning algorithms: in particular take a look at [Nearest Neighbor](https://scikit-learn.org/stable/modules/neighbors.html) and [Support Vector Machine (SVM)](https://scikit-learn.org/stable/modules/svm.html#regression) regression algorithms. Don't spend too much time tweaking the default hyperparameters. The goal is to shortlist two or three promising models for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = model_selection.cross_val_score(neighbors.KNeighborsRegressor(),\n",
    "                                             X=preprocessed_training_features,\n",
    "                                             y=training_target_df,\n",
    "                                             cv=10,\n",
    "                                             scoring=\"neg_mean_squared_error\",\n",
    "                                             n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_rmses = np.sqrt(-knn_scores)\n",
    "display_rmses(knn_rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_scores = model_selection.cross_val_score(svm.SVR(),\n",
    "                                             X=preprocessed_training_features,\n",
    "                                             y=training_target_df,\n",
    "                                             cv=10,\n",
    "                                             scoring=\"neg_mean_squared_error\",\n",
    "                                             n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rmses = np.sqrt(-svr_scores)\n",
    "display_rmses(svr_rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune your models\n",
    "\n",
    "Most common approach to tuning a model is to manually fiddle with the hyperparameters until you find a great combination of hyperparameter values. Needless to day, this approach to model tuning is *very* tedious and not at all scientific. We can do much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Simplest approach is to use Scikit-Learn’s [`model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). All you need to do is tell it which hyperparameters you want it to experiment with and what values to try out. The `model_selection.GridSearchCV` class will then use cross-validation to evaluate all the possible combinations of hyperparameter values and return the best scoring set of hyperparameters according to your specified metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = [\n",
    "    {'n_estimators': [10, 100], 'max_features': [\"auto\", \"sqrt\", \"log2\"]}, # 2 * 3 = 6 parameter combinations to try\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 4, 8]}, # 1 * 2 * 3 = 6 parameter combinations to try\n",
    "]\n",
    "\n",
    "_random_state = np.random.RandomState(42)\n",
    "random_forest_regressor = ensemble.RandomForestRegressor(random_state=_random_state)\n",
    "\n",
    "grid_search_cv = model_selection.GridSearchCV(random_forest_regressor,\n",
    "                                              parameter_grid,\n",
    "                                              cv=5,\n",
    "                                              scoring='neg_mean_squared_error',\n",
    "                                              return_train_score=True,\n",
    "                                              n_jobs=5,\n",
    "                                              verbose=10)\n",
    "\n",
    "grid_search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = grid_search_cv.fit(preprocessed_training_features, training_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for the best parameters\n",
    "(-grid_search_cv.best_score_)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_estimator_ is trained with the values from best_params_\n",
    "grid_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should save every model you experiment with so that you can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to more easily compare scores across model types and compare the types of errors they make. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import time\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ = joblib.dump(grid_search_cv, f\"../results/models/grid-search-cv-random-forest-regressor-{timestamp}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference here is how you would reload the trained model from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_grid_search_cv = joblib.load(f\"../results/models/grid-search-cv-random-forest-regressor-{timestamp}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with grid_search_cv.best_params_\n",
    "reloaded_grid_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search\n",
    "\n",
    "The grid search approach is fine when you are exploring relatively few combinations but when the hyperparameter search space is large it is often preferable to use [`model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) instead. Instead of trying out all possible combinations, `model_selection.RandomizedSearchCV` evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits.\n",
    "\n",
    "* More efficient exploration of the hyperparameter space.\n",
    "* More control over the computing budget you want to allocate to hyperparameter search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "_param_distributions = {\n",
    "    \"n_estimators\": stats.geom(p=0.01),\n",
    "    \"min_samples_split\": stats.beta(a=1, b=99),\n",
    "    \"min_samples_leaf\": stats.beta(a=1, b=999),\n",
    "}\n",
    "\n",
    "_random_state = np.random.RandomState(42)\n",
    "random_forest_regressor = ensemble.RandomForestRegressor(random_state=_random_state)\n",
    "\n",
    "randomized_search_cv = model_selection.RandomizedSearchCV(\n",
    "    random_forest_regressor,\n",
    "    param_distributions=_param_distributions,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    random_state=_random_state,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    n_jobs=5,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "randomized_search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = randomized_search_cv.fit(preprocessed_training_features, training_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for the best parameters\n",
    "(-randomized_search_cv.best_score_)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ = joblib.dump(randomized_search_cv.best_estimator_, f\"../results/models/randomized-search-cv-random-forest-regressor-{_timestamp}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search and randomized search are the two easiest ways to get started with hyperparameter optimization (HPO) within Scikit-Learn. However, increasingly I finfd myself using [Optuna](https://optuna.org/) for my HPO workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Best Models and Their Errors \n",
    "\n",
    "You will often gain good insights on the problem by inspecting the best models. For example, the `ensemble.RandomForestRegressor` can indicate the relative importance of each attribute for making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = (randomized_search_cv.best_estimator_\n",
    "                             .feature_importances_)\n",
    "_index = preprocessed_training_features_df.columns\n",
    "feature_importances = pd.Series(_data, index=_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like only one of the categories of `ocean_proximity` is useful. Based on this information, I might go back and re-encode `ocean_proximity` to be a binary indicator that takes the value of `1` if the category is either `ISLAND`, `NEAR_BAY`, or `NEAR OCEAN` and `0` if the value is `INLAND`. The would reduce the number of features and speed up computation for some machine learning models.\n",
    "\n",
    "You should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem (adding extra features or getting rid of uninformative ones, cleaning up outliers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_true = (training_target_df.values\n",
    "                             .ravel())\n",
    "_y_pred = (randomized_search_cv.best_estimator_\n",
    "                               .predict(preprocessed_training_features))\n",
    "_prediction_errors = _y_true - _y_pred # positive prediction error indicates model under-predicts housing prices!\n",
    "preprocessed_training_features_df[\"prediction_errors\"] = _prediction_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction errors have lots of outliers\n",
    "\n",
    "If your predictions errors exhibit lots of outliers, then you can inspect which training data samples are the ones for which the model makes poor predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "_ = preprocessed_training_features_df.loc[:, \"prediction_errors\"].plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census block groups for which model under-predicts housing prices\n",
    "(preprocessed_training_features_df.sort_values(\"prediction_errors\", ascending=False)\n",
    "                                  .head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census block groups for which model over-predicts housing prices\n",
    "(preprocessed_training_features_df.sort_values(\"prediction_errors\", ascending=False)\n",
    "                                  .tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the geographical distribution of prediction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "_color = preprocessed_training_features_df.loc[:, \"prediction_errors\"] / 10000\n",
    "_cmap = plt.get_cmap(\"viridis\")\n",
    "_ = preprocessed_training_features_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", c=_color, cmap=_cmap, ax=ax, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring how prediction errors vary with median income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "_ = preprocessed_training_features_df.plot(kind=\"scatter\", x=\"median_income\", y=\"prediction_errors\", ax=ax, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your system on the test dataset\n",
    "\n",
    "After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.read_csv(\"../data/housing/testing.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_engineered_features_df = engineer_features(testing_df)\n",
    "cleaned_testing_df = clean_dataset(with_engineered_features_df)\n",
    "testing_features_df = cleaned_testing_df.drop(\"median_house_value\", axis=1, inplace=False)\n",
    "testing_target_df = cleaned_testing_df.loc[:, \"median_house_value\"]\n",
    "preprocessed_testing_features = preprocessing_pipeline.transform(testing_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = randomized_search_cv.best_estimator_.predict(preprocessed_testing_features)\n",
    "np.sqrt(metrics.mean_squared_error(testing_target_df, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just marginally better than the model currently in production? You might want to have an idea of how precise this estimate is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of computing an estimate of the confidence interval for the test set error\n",
    "confidence = 0.95\n",
    "squared_errors = (testing_target_df - predictions)** 2\n",
    "_interval = (stats.t\n",
    "                  .interval(confidence,\n",
    "                            squared_errors.size - 1,\n",
    "                            loc=squared_errors.mean(),\n",
    "                            scale=stats.sem(squared_errors)))\n",
    "np.sqrt(_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did a lot of hyperparameter tuning, the performance will usually be slightly worse than what you measured using cross-validation (because your system ends up fine-tuned to perform well on the validation data and will likely not perform as well on unknown datasets). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
